{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "agreed-examination",
   "metadata": {},
   "source": [
    "# NLP Series 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-brother",
   "metadata": {},
   "source": [
    "In our last notebook(NLP Series 1) we saw how Lemmatization and Stemming works. Now let us look at Bag of Words intuition, and TF-IDF intuition and try and understand how to perform it on a paragraph, and as well the underlying benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "starting-reasoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   We need to clean-up this message. It consists of several steps, which we will follow as we proceed further. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     . I am trying and adding another sentence to make it more real-life like. There is another sentence to give it more weight. How, do you feel about it now? Is there anything that can be further added to make it look like a paragraph. At this stage, I think I have few sentences and I can proceed with the process. /  \n"
     ]
    }
   ],
   "source": [
    "# sample text\n",
    "original_text = \"   We need to clean-up this message. It consists of several steps, which we will follow as we proceed further. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     . I am trying and adding another sentence to make it more real-life like. There is another sentence to give it more weight. How, do you feel about it now? Is there anything that can be further added to make it look like a paragraph. At this stage, I think I have few sentences and I can proceed with the process. /  \"\n",
    "print(original_text)\n",
    "process_text = original_text # use this to process / cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "invisible-preference",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "import nltk #library\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re #regular expression\n",
    "from nltk.corpus import stopwords # Import Stop words\n",
    "from nltk.stem.porter import PorterStemmer # Stemming or you can use Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "naval-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer() # Create object for Stemming\n",
    "wordnet = WordNetLemmatizer() # object for lemmatization\n",
    "sentences = nltk.sent_tokenize(process_text) # convert paragraph into sentences\n",
    "\n",
    "# Create list to store cleanup sentences\n",
    "corpus = []\n",
    "\n",
    "#Clean-up now done using a for-loop instead of series of statements. In real-life external scripts can also be leveraged to clean-up\n",
    "# iterate through each sentence\n",
    "for i in range(len(sentences)):\n",
    "    cleaned = re.sub('[^a-zA-Z]', ' ', sentences[i]) # clean-up punctuation, unnecessary characters other than a-z|A-Z with space\n",
    "    cleaned = cleaned.lower() # convert sentence to lower case Why? Casing may interpret Ball and ball separetely\n",
    "    cleaned = cleaned.split() #list of words\n",
    "    cleaned = [ps.stem(word) for word in cleaned if not word in set(stopwords.words('english'))] #If not word not present in Stop Words, Stemming needs to be done\n",
    "    cleaned = ' '.join(cleaned) # cleaned\n",
    "    corpus.append(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "manufactured-chapel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need clean messag\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "animal-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer() # Create object for Stemming\n",
    "wordnet = WordNetLemmatizer() # object for lemmatization\n",
    "sentences = nltk.sent_tokenize(process_text) # convert paragraph into sentences\n",
    "\n",
    "# Create list to store cleanup sentences\n",
    "corpus = []\n",
    "\n",
    "#Clean-up now done using a for-loop instead of series of statements. In real-life external scripts can also be leveraged to clean-up\n",
    "# iterate through each sentence\n",
    "for i in range(len(sentences)):\n",
    "    cleaned = re.sub('[^a-zA-Z]', ' ', sentences[i]) # clean-up punctuation, unnecessary characters other than a-z|A-Z with space\n",
    "    cleaned = cleaned.lower() # convert sentence to lower case Why? Casing may interpret Ball and ball separetely\n",
    "    cleaned = cleaned.split() #list of words\n",
    "    cleaned = [ps.stem(word) for word in cleaned if not word in set(stopwords.words('english'))] #If not word not present in Stop Words, Stemming needs to be done\n",
    "    cleaned = ' '.join(cleaned) # cleaned\n",
    "    corpus.append(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-genetics",
   "metadata": {},
   "source": [
    "# Create Bag of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "egyptian-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray() #BoW on the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "increased-kelly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X) # Bag Of Words Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-convergence",
   "metadata": {},
   "source": [
    "If you look above, you can see how the BoW vector is generated above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-chorus",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-pontiac",
   "metadata": {},
   "source": [
    "## TF - Term Frequency | IDF - Inverse Document Frequency\n",
    "Formulae - TF \n",
    "**(No. of representive words in a sentence / Total no. of words in sentence)**\n",
    "\n",
    "Formulae - IDF \n",
    "**(log(No. of Sentences / No. of sentences containing words))**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-lease",
   "metadata": {},
   "source": [
    "Lets use the same sample paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abstract-northwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   We need to clean-up this message. It consists of several steps, which we will follow as we proceed further. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     . I am trying and adding another sentence to make it more real-life like. There is another sentence to give it more weight. How, do you feel about it now? Is there anything that can be further added to make it look like a paragraph. At this stage, I think I have few sentences and I can proceed with the process. /  \n"
     ]
    }
   ],
   "source": [
    "print(original_text)\n",
    "process_tf_idf_text = original_text # use this to process / cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-stephen",
   "metadata": {},
   "source": [
    "Similar techniques can be used to clean-up the text, as we saw earlier in BoW. We already have the libraries imported, so we do not need to import again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "unlikely-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "pStem = PorterStemmer() # Create object for Stemming\n",
    "wordnet = WordNetLemmatizer() # object for lemmatization\n",
    "sentences = nltk.sent_tokenize(process_tf_idf_text) # convert paragraph into sentences\n",
    "\n",
    "# Create list to store cleanup sentences\n",
    "corpus = []\n",
    "\n",
    "#Clean-up now done using a for-loop instead of series of statements. In real-life external scripts can also be leveraged to clean-up\n",
    "# iterate through each sentence\n",
    "for i in range(len(sentences)):\n",
    "    process = re.sub('[^a-zA-Z]', ' ', sentences[i]) # clean-up punctuation, unnecessary characters other than a-z|A-Z with space\n",
    "    process = process.lower() # convert sentence to lower case Why? Casing may interpret Ball and ball separetely\n",
    "    process = process.split() #list of words\n",
    "    process = [wordnet.lemmatize(word) for word in process if not word in set(stopwords.words('english'))] #If not word not present in Stop Words, Stemming needs to be done\n",
    "    process = ' '.join(process) # cleaned\n",
    "    corpus.append(process)\n",
    "    \n",
    "# Create the TF-IDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer() # Here we are using TFidfVectorizer instead of CountVectorizer(BoW)\n",
    "XVal = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "announced-breeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.57735027 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.57735027\n",
      "  0.57735027 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.46061063 0.         0.46061063 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.38903907 0.         0.         0.\n",
      "  0.46061063 0.         0.         0.46061063 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.36419547 0.         0.         0.36419547\n",
      "  0.         0.         0.         0.         0.         0.36419547\n",
      "  0.         0.26745392 0.         0.         0.36419547 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.36419547 0.         0.         0.36419547 0.36419547\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.392071   0.         0.33114941 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.392071   0.28792485 0.         0.33114941 0.         0.\n",
      "  0.         0.         0.         0.         0.392071   0.28792485\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.392071   0.        ]\n",
      " [0.         0.         0.         0.46831599 0.         0.\n",
      "  0.         0.         0.         0.         0.55447213 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.40718723\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.55447213]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.43632467 0.         0.         0.         0.43632467 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.32042338 0.43632467 0.36852676 0.         0.\n",
      "  0.         0.43632467 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.40957007 0.48491872 0.         0.35610935\n",
      "  0.         0.         0.48491872 0.         0.         0.\n",
      "  0.48491872 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(XVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-plasma",
   "metadata": {},
   "source": [
    "If you look at the Vector above, X starts to show decimal value and provides *Importance* to words based on TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-hayes",
   "metadata": {},
   "source": [
    "In the next series, we will look at Unigrams, Bigrams and n-Grams and understand why it is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-switch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
