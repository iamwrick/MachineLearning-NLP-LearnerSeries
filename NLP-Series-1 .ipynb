{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "metropolitan-reliance",
   "metadata": {},
   "source": [
    "# NLP Series 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-homework",
   "metadata": {},
   "source": [
    "Whenever we are referring to some MachineLearning problem, the basic requirement is to have quality data, which we can feed to the Model to Train it, and based on the Training we can make predictions. Now, data can have various shape, size, construct and complexity. Data can be in the form of Text, as we are looking at NLP, it could also be a categorical features etc. Data preprocessing techniques are needed to process the data, even before model building. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-canberra",
   "metadata": {},
   "source": [
    "Let us now take an example, to clearly understand the concept. Let's choose the publicly available dataset of Amazon product reviews to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-tribe",
   "metadata": {},
   "source": [
    "All over the world, people use Amazon to buy products. Based on individual experiences, some people provide feedback and/or review of the product. Now, how machines can really understand whether the review provided is Positive or Negative? That's where Natural Language Processing comes into play. Various Text processing techniques are used to handle these scenarios. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-attachment",
   "metadata": {},
   "source": [
    "We will refer to NLTK open source library to explain few of the concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-pregnancy",
   "metadata": {},
   "source": [
    "\n",
    "## Text Processing\n",
    "\n",
    "In this notebook, we go over some simple techniques to clean and prepare text data for modeling with machine learning.\n",
    "\n",
    "1. <a href=\"#1\">Simple text cleaning processes</a>\n",
    "2. <a href=\"#2\">Lexicon-based text processing</a>\n",
    "    * Tokenization\n",
    "    * Lemmatization\n",
    "    * Stemming\n",
    "    * Stop words removal \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-command",
   "metadata": {},
   "source": [
    "## 1. <a name=\"1\">Simple text cleaning processes</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "In this section, we will try some text cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test we will be cleaning tokenizing, lemmatizing, stemming and look for stop words\n",
    "original_text = \"   We need to clean-up this message. It consists of several steps, which we will follow as we proceed further. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     . I am trying and adding another sentence to make it more real-life like. There is another sentence to give it more weight. How, do you feel about it now? Is there anything that can be further added to make it look like a paragraph. At this stage, I think I have few sentences and I can proceed with the process. /  \"\n",
    "print(original_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-vancouver",
   "metadata": {},
   "source": [
    "# Clean-up Approach"
   ]
  },
  {
   "cell_type": "raw",
   "id": "exact-samoa",
   "metadata": {},
   "source": [
    "# 1. Simple Cleaning Approach - Remove whitespaces, punctuation, html, tags, markups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-patient",
   "metadata": {},
   "source": [
    "# Assign original_text to text to keep a backup of the original entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let get rid off the trailing whitespaces, as it doesn't add any value to the ML process\n",
    "text = original_text\n",
    "text = text.strip()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-pricing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's get rid off the HTML tags/markups\n",
    "import re\n",
    "\n",
    "text = re.compile('<.*?>').sub('', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's now remove the punctuations and replace it with spaces\n",
    "import re, string\n",
    "\n",
    "text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all to lowercase\n",
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets remove the extra spaces and tabs\n",
    "import re\n",
    "\n",
    "text = re.sub('\\s+', ' ', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-ottawa",
   "metadata": {},
   "source": [
    "# 2. Lexicon Based Cleaning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-huntington",
   "metadata": {},
   "source": [
    "Lexicon based methods are usually applied after the common text processing methods. It helps to put words into a similar format that will also enhace similarities (if any) between sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-storm",
   "metadata": {},
   "source": [
    "To do that, we need to download few libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-senate",
   "metadata": {},
   "source": [
    "# a) Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-shade",
   "metadata": {},
   "source": [
    "Simply speaking, Tokenization is the process of converting text and/or documents into small parts by white spaces and punctuation. Let us see what happens to the original_text, when we apply Tokenization to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences. Either you can tokenize sentences or words.\n",
    "# You will see the sentence is now a unique list. Depending on the paragraph(no of sentences), you may end up \n",
    "# with multiple lists\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words. Remember based on the paragraph/sentence, there could be repetition of words\n",
    "words = nltk.word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-fundamental",
   "metadata": {},
   "source": [
    "# b) Stemming and Stop Word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-essay",
   "metadata": {},
   "source": [
    "Stemming is the set of rules to dice and slice the words to make more generic sense. Example - Going, Goes, Gone -> Go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-error",
   "metadata": {},
   "source": [
    "Why it is important ? The Stem word provides more contextual perspective to the process. \"Go\" have more significance over \"Going, Goes, Gone\" in this contextual understanding. Stemming although doesn't necessary mean it will convert word to a human interpretable form. Example - Finally, Final, Finalized would be \"fina\", which doesn't have any meaning. Wheras, Lemmatization ensures there is a human understandable meaning of the word. So Finally, Final, Finalized would be \"final\". But at the same time it takes more time to perform Lemmatization Vs Stemming. So which one to use where? It will depend on Use Cases. Example - Sentiment analysis, Spam classifier etc. we might not need to understand the base word(english meaning perspective), but for Use cases like Chat Bots, FAQ, Q & A we might need a complete meaning of the word, and hence Lemmatization would be more appropriate for a meaningful representaion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use a tokenizer and stemmer from the NLTK library\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Let's get a list of stop words from the NLTK library\n",
    "snowstemmer = SnowballStemmer('english')\n",
    "\n",
    "#Stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i]) # words in sentence(s)\n",
    "    words = [snowstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-scanning",
   "metadata": {},
   "source": [
    "So you can see that all words does not have a meaningful representation of the word in english. Let us now look at Lemmatization technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-bronze",
   "metadata": {},
   "source": [
    "# c) Lemmatization and Stop Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary functions\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Lemmatizer Object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i]) # words in sentence(s)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-thompson",
   "metadata": {},
   "source": [
    "You can see Lemmatization produces far better results compared to Stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-taxation",
   "metadata": {},
   "source": [
    "Let's now continue to focus on how BoW, TF-IDF, Word2Vec is done in real-life. For that, please check out the NLP-Series-2 notebook......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-bennett",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
