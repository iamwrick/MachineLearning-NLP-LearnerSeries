{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "logical-check",
   "metadata": {},
   "source": [
    "# NLP Series 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-contrary",
   "metadata": {},
   "source": [
    "In this notebook we will look at Word2Vec. We previously looked at BoW, TF-IDF. Lets see how we can apply Word2Vec\n",
    "using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-williams",
   "metadata": {},
   "source": [
    "But before understanding the concept of BoW and TF-IDF, let's try and understand what are the challenges we often see with those approaches\n",
    "\n",
    "1. BoW & TF-IDF - For both the approaches the semantic information is not stored, and on top for TF-IDF the *uncommon* words gets **importance**\n",
    "\n",
    "2. This amy lead to **overfitting**\n",
    "\n",
    "*Note - Semantic information: Order and relationship between words are not stored\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-reynolds",
   "metadata": {},
   "source": [
    "## What is Word2Vec ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-threat",
   "metadata": {},
   "source": [
    "Each word is represented as a vector of 32 or more dimension instead of a single number. The real advantage with this approach is, that the semantic information and relationship between words are preserved.\n",
    "\n",
    "Example - Let's assume we have 3 words in a sentence - Boy, Girl, soccer. If you refer to the 2Dimensional representation, Boy & Girl will have less distance between them as compared to soccer. Reason - Boy and Girl has a relationship, which is more close compared to soccer. Now, if I had a word, football, the relationship between and soccer and football will be more close as compared to Boy and Girl. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-costume",
   "metadata": {},
   "source": [
    "## Essential steps to create Word2Vec\n",
    "1. Tokenization of the sentences\n",
    "2. Histograms\n",
    "3. Leverage most frequent words\n",
    "4. Create a matrix of all unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim - we will leverage gensim from Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # import regular expression\n",
    "\n",
    "#import library\n",
    "import nltk #library\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "married-offset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   We need to clean-up this message. It consists of several steps, which we will follow as we proceed further. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     . I am trying and adding another sentence to make it more real-life like. There is another sentence to give it more weight. How, do you feel about it now? Is there anything that can be further added to make it look like a paragraph. At this stage, I think I have few sentences and I can proceed with the process. /  \n"
     ]
    }
   ],
   "source": [
    "# sample text\n",
    "original_text = \"   We need to clean-up this message. It consists of several steps, which we will follow as we proceed further. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     . I am trying and adding another sentence to make it more real-life like. There is another sentence to give it more weight. How, do you feel about it now? Is there anything that can be further added to make it look like a paragraph. At this stage, I think I have few sentences and I can proceed with the process. /  \"\n",
    "print(original_text)\n",
    "process_text = original_text # use this to process / cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-science",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-matter",
   "metadata": {},
   "source": [
    "1. Apply Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "frequent-necklace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " we need to cleanup this message it consists of several steps which we will follow as we proceed further it may involve some things like     adjacent spaces and tabs  i am trying and adding another sentence to make it more reallife like there is another sentence to give it more weight how do you feel about it now is there anything that can be further added to make it look like a paragraph at this stage i think i have few sentences and i can proceed with the process  \n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the data using regular expression\n",
    "process_text = re.sub(r'\\[[0-9]*\\]',' ',process_text)\n",
    "process_text = re.sub(r'\\s+',' ',process_text)\n",
    "process_text = process_text.lower()\n",
    "process_text = re.sub(r'\\d',' ',process_text)\n",
    "process_text = re.sub(r'\\s+',' ',process_text)\n",
    "process_text = re.compile('<.*?>').sub('', process_text) # Remove HTML tags/markups\n",
    "process_text = re.sub(r'[^\\w\\s]', '', process_text) # remove punctions using regex\n",
    "print(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "broad-coordinator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' we need to cleanup this message it consists of several steps which we will follow as we proceed further it may involve some things like     adjacent spaces and tabs  i am trying and adding another sentence to make it more reallife like there is another sentence to give it more weight how do you feel about it now is there anything that can be further added to make it look like a paragraph at this stage i think i have few sentences and i can proceed with the process']\n"
     ]
    }
   ],
   "source": [
    "# Preparing the dataset. Convert text to sentences\n",
    "sentences = nltk.sent_tokenize(process_text)\n",
    "print(sentences)\n",
    "\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences] #convert each sentences to convert to word.\n",
    "# Why? There are STOP words, which needs to be removed as it doesn't add much syntax\n",
    "\n",
    "#remove STOP words in English and only add the rest\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "statutory-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Word2Vec model\n",
    "# use Word2Vec from gensim library. Pass the whole sentence to it and skip words which is present < 1. In real-life\n",
    "# you may chose min_count = 2/5 etc. Since we have a very small example sentence, I am choosing 1 here\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-empty",
   "metadata": {},
   "source": [
    "At its core, word2vec model parameters are stored as matrices (NumPy arrays). Each array is #vocabulary (controlled by min_count parameter) times #size (size parameter) of floats (single precision aka 4 bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "indoor-tuesday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0021954  -0.00970765  0.00929529  0.00203197 -0.00116118 -0.00550371\n",
      " -0.00850983 -0.00990348  0.00894438 -0.00249943  0.00459238 -0.00451736\n",
      "  0.00995806  0.00365472  0.00102569 -0.0040441   0.00121062 -0.0026468\n",
      "  0.00735074  0.00447825  0.00098668  0.00348309  0.00371435 -0.00678538\n",
      "  0.00893076  0.00173623 -0.00578747  0.00865668 -0.00129148  0.00818704\n",
      " -0.00150687  0.00698765  0.0027264  -0.00435973 -0.00374793  0.00919178\n",
      "  0.00159187 -0.00600701  0.00034962 -0.0019624   0.00158453 -0.00771732\n",
      "  0.00738409  0.00131222  0.0078752   0.00445304 -0.00439387  0.00375268\n",
      " -0.00063848 -0.00985625  0.00824213  0.00964937  0.00965339 -0.00379981\n",
      " -0.00844962  0.0048277  -0.00765548  0.00853205  0.00276025  0.00560334\n",
      "  0.00611649  0.00046481 -0.0020949   0.00077317  0.00983596 -0.00712454\n",
      " -0.00155246 -0.00235854  0.00486601  0.00645004 -0.00413204  0.00362367\n",
      " -0.00448113  0.00327047  0.00816847  0.00362861 -0.00457281 -0.00300916\n",
      "  0.0078664   0.00960406  0.00581709 -0.00327404 -0.00182528 -0.00625739\n",
      " -0.00429558  0.00337436 -0.00648775 -0.00661616  0.00811142  0.00950546\n",
      "  0.00814058  0.00151043 -0.00879589 -0.00759717  0.00157208 -0.00952763\n",
      " -0.00741021  0.00202412 -0.00292117 -0.00915701]\n"
     ]
    }
   ],
   "source": [
    "#Finding word vectors\n",
    "vector = model.wv['need']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "anticipated-lightweight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('look', 0.19099311530590057), ('give', 0.18883724510669708), ('message', 0.16090546548366547), ('tabs', 0.16009476780891418), ('consists', 0.13768422603607178), ('weight', 0.12850235402584076), ('cleanup', 0.12295003235340118), ('trying', 0.08621159940958023), ('sentence', 0.0679503083229065), ('need', 0.03552912175655365)]\n"
     ]
    }
   ],
   "source": [
    "# Most similar words\n",
    "similar = model.wv.most_similar('proceed')\n",
    "print(similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-tuesday",
   "metadata": {},
   "source": [
    "There are more ways to train word vectors in Gensim than just Word2Vec. You can also refer to Doc2Vec, FastText."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-poetry",
   "metadata": {},
   "source": [
    "## Which one to choose? \n",
    "It is a complex question, and I have seen people use both TF-IDF, Word2Vec and go with some level of experience and intuition. Let's see if we can look at the mathematics of may be which one to choose - \n",
    "First of all, at this stage we have already **extracted** the **terms** from a sentence or set of documents. Our goal now is to produce a **Reference Term Vector(RTV)** that characterizes the **important** **Term** within the corpus. There are two set of Algorthims to consider - \n",
    "1. Pre-2000 : TF-IDF\n",
    "2. Post-2000 : Word2Vec | Doc2Vec. *WordVec - works on a single word in a document whereas Doc2Vec works on the whole set of words within a document\n",
    "\n",
    "BERT is another algorithm which we will look at a later stage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-there",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
